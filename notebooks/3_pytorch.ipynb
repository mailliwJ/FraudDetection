{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/creditcard.csv')\n",
    "data.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(data, test_size=0.8, stratify=data['Class'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.2, stratify=temp_df['Class'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create custom dataset class `FraudDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, DataFrame):\n",
    "        super().__init__()\n",
    "        df = DataFrame\n",
    "        self.data = df.to_numpy().astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __input_size__(self):\n",
    "        return self.data.shape[1] -1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = tensor(self.data[idx, :-1], dtype=torch.float32)\n",
    "        y = tensor(self.data[idx, -1], dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "# Instatiate FraudDataset objects for each dataset \n",
    "train_set = FraudDataset(train_df)\n",
    "val_set = FraudDataset(val_df)\n",
    "test_set = FraudDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_set,batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create NN architectue class `FraudNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FraudNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.dropout(x, p=0.2)\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "input_size = train_set.__input_size__()\n",
    "nnet = FraudNN(input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0033, Validation Loss: 0.0157\n",
      "Epoch [2/10], Loss: 0.0043, Validation Loss: 0.0206\n",
      "Epoch [3/10], Loss: 0.0061, Validation Loss: 0.0161\n",
      "Epoch [4/10], Loss: 0.0027, Validation Loss: 0.0190\n",
      "Epoch [5/10], Loss: 0.0034, Validation Loss: 0.0238\n",
      "Epoch [6/10], Loss: 0.0060, Validation Loss: 0.0174\n",
      "Epoch [7/10], Loss: 0.0023, Validation Loss: 0.0158\n",
      "Epoch [8/10], Loss: 0.0027, Validation Loss: 0.0157\n",
      "Epoch [9/10], Loss: 0.0019, Validation Loss: 0.0184\n",
      "Epoch [10/10], Loss: 0.0023, Validation Loss: 0.0160\n",
      "\n",
      "-------------------------------------------------------\n",
      "Best Validation Loss: Epoch: 1, Validation Loss: 0.0157\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Loss Function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 2. Define Optimizer\n",
    "optimizer = optim.Adam(nnet.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Define number of epochs and initialize variable for best validation loss metric\n",
    "epochs = 10\n",
    "best_val_loss = [None, float('inf')]\n",
    "\n",
    "# 4. Define training and validation loops\n",
    "for epoch in range(epochs):\n",
    "    # Set model in train mode\n",
    "    nnet.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nnet(X)\n",
    "        loss = criterion(outputs, y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # 5. Set model in eval mode \n",
    "    nnet.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            val_outputs = nnet(X)\n",
    "            loss = criterion(val_outputs, y.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Save the best model metric\n",
    "    if val_loss < best_val_loss[1]:\n",
    "        best_val_loss[0] = epoch\n",
    "        best_val_loss[1] = val_loss\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "print()\n",
    "string = f'Best Validation Loss: Epoch: {best_val_loss[0]+1}, Validation Loss: {best_val_loss[1]:.4f}'\n",
    "print('-'*len(string))\n",
    "print(string)\n",
    "print('-'*len(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9993, Test Precision: 0.8636, Test Recall: 0.7215\n"
     ]
    }
   ],
   "source": [
    "acc = Accuracy(task='binary')\n",
    "precision = Precision(task='binary')\n",
    "recall = Recall(task='binary')\n",
    "\n",
    "nnet.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        outputs = nnet(X)\n",
    "        preds = (outputs >= 0.55).float()\n",
    "        acc(preds, y.view(-1,1))\n",
    "        precision(preds, y.view(-1,1))\n",
    "        recall(preds, y.view(-1,1))\n",
    "  \n",
    "\n",
    "test_accuracy = acc.compute()\n",
    "test_precision = precision.compute()\n",
    "test_recall = recall.compute()\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Overfitting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fighting overfitting\n",
    "- dropout=0.2\n",
    "- learning_rate=1e-3\n",
    "- weight_decay=1e-4\n",
    "- weight decay takes values between 0 and 1.\n",
    "- typically small values like 1e-3.\n",
    "- adds penalty to loss function to discourage large weights and biases.\n",
    "- proportional to the current value of the weight and subtracted from the gradient.\n",
    "- higher the value of the parameter, the less likely the model is to overfit.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "## Maximizing performance\n",
    "- overfit the training set\n",
    "- reduce overfitting\n",
    "- fine-tune hyperparameters\n",
    "\n",
    "### 1. Overfitting Training set\n",
    "- modify training loop to overfit a single data point (batch size = 1)\n",
    "```Python\n",
    "features, labels = next(iter(trainloader))\n",
    "for i in range(1e-3):\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "- Should give an accuracy of 1.0 and a loss of 0\n",
    "- Helps finding bus in code\n",
    "- ***goal***: minimize training loss\n",
    "- create large enough model\n",
    "- hyperparameters kept to defaults for now\n",
    "\n",
    "### 2. Reduce Overfitting\n",
    "- ***Goal***: Maximize the validation accuracy\n",
    "- experiment with:\n",
    "    - Dropout\n",
    "    - Data augmentation\n",
    "    - weight decay\n",
    "    - Reducing model capacity\n",
    "- keep track of each hyperparameter set and corresponding accuracy / metric.\n",
    "- plot each experiment against the default setting train/val curves\n",
    "\n",
    "### 3. Fine-Tuning Hyperparameters\n",
    "- Grid Search:\n",
    "    - Usually done on the optimizer hyperparameters\n",
    "    - Uses values of the parameters at a constant inverval\n",
    "    - Eg. Every momentum value between 0.85 and 0.99 with a constant interval\n",
    "    - \n",
    "```Python\n",
    "for factor in range(2,6):\n",
    "    lr = 10**-factor\n",
    "for val in np.arange(0.85, 1.00, 0.01):\n",
    "    momentum = val\n",
    "```\n",
    "- Random Search:\n",
    "    - Randomly samples parameters between intervals.\n",
    "    - Quicker, and possibly better results as searches a less restricted sapce\n",
    "```Python\n",
    "factor = np.random.uniform(2,6)\n",
    "lr = 10**-factor\n",
    "val = np.random.uniform(0.85, 1.00)\n",
    "momentum = val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
