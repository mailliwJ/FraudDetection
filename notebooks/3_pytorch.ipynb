{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Set seeding for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/creditcard.csv')\n",
    "data.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(data, test_size=0.8, stratify=data['Class'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.2, stratify=temp_df['Class'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create custom dataset class `FraudDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, DataFrame):\n",
    "        super().__init__()\n",
    "        df = DataFrame\n",
    "        self.data = df.to_numpy().astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __input_size__(self):\n",
    "        return self.data.shape[1] -1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = tensor(self.data[idx, :-1], dtype=torch.float32)\n",
    "        y = tensor(self.data[idx, -1], dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "# Instatiate FraudDataset objects for each dataset \n",
    "train_set = FraudDataset(train_df)\n",
    "val_set = FraudDataset(val_df)\n",
    "test_set = FraudDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create DataLoaders using `DataLoader` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "val_loader = DataLoader(val_set,batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create NN architectue class `FraudNN`\n",
    "\n",
    "#### Notes on NN architecture\n",
    "\n",
    "- Larger layers help to capture complex patterns in the dataset.\n",
    "- Consider using leaky_relu instead of ReLU activation on larger layers as allows a small gradient to flow even for negative values, which can lead to better convergence and prevent dying ReLU where neurons get 'stuck' during training.\n",
    "    - Trialed this and leaky_relu was actually worse for this dataset.\n",
    "    - ReLU gave reduced overfitting more than leaky_relu\n",
    "- Incrementally reducing dropout after largers layer to smaller layers helps to reduce overfitting and maintain learning capactiy without excessive regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FraudNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.dropout(x, p=0.3)\n",
    "        x = nn.functional.elu(self.fc2(x))\n",
    "        x = nn.functional.dropout(x, p=0.2)\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = nn.functional.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "input_size = train_set.__input_size__()\n",
    "nnet = FraudNN(input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Early Stopping class `EarlyStopping`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.0420, Validation Loss: 0.0311\n",
      "Epoch [2/1000], Loss: 0.0176, Validation Loss: 0.0166\n",
      "Epoch [3/1000], Loss: 0.0108, Validation Loss: 0.0081\n",
      "Epoch [4/1000], Loss: 0.0108, Validation Loss: 0.0111\n",
      "Epoch [5/1000], Loss: 0.0092, Validation Loss: 0.0079\n",
      "Epoch [6/1000], Loss: 0.0059, Validation Loss: 0.0083\n",
      "Epoch [7/1000], Loss: 0.0060, Validation Loss: 0.0067\n",
      "Epoch [8/1000], Loss: 0.0048, Validation Loss: 0.0060\n",
      "Epoch [9/1000], Loss: 0.0041, Validation Loss: 0.0095\n",
      "Epoch [10/1000], Loss: 0.0058, Validation Loss: 0.0129\n",
      "Epoch [11/1000], Loss: 0.0044, Validation Loss: 0.0123\n",
      "Epoch [12/1000], Loss: 0.0069, Validation Loss: 0.0119\n",
      "Epoch [13/1000], Loss: 0.0035, Validation Loss: 0.0098\n",
      "Epoch [14/1000], Loss: 0.0048, Validation Loss: 0.0108\n",
      "Epoch [15/1000], Loss: 0.0038, Validation Loss: 0.0111\n",
      "Epoch [16/1000], Loss: 0.0052, Validation Loss: 0.0123\n",
      "Epoch [17/1000], Loss: 0.0037, Validation Loss: 0.0099\n",
      "Epoch [18/1000], Loss: 0.0078, Validation Loss: 0.0163\n",
      "\n",
      "Early stopping triggered at epoch 18\n",
      "Best Validation Loss: 0.0060\n",
      "Optimal threshold based on validation F1-Score: 0.34\n"
     ]
    }
   ],
   "source": [
    "# 1. Define Loss Function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 2. Define Optimizer\n",
    "optimizer = optim.Adam(nnet.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Define Early Stopping\n",
    "early_stopper = EarlyStopping(patience=10, delta = 0.0001)\n",
    "\n",
    "# 4. Define number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# 4. Define training and validation loops\n",
    "for epoch in range(epochs):\n",
    "    # Set model in train mode\n",
    "    nnet.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nnet(X)\n",
    "        loss = criterion(outputs, y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # 5. Set model in eval mode \n",
    "    nnet.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            val_outputs = nnet(X)\n",
    "            loss = criterion(val_outputs, y.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Apply Early Stopping\n",
    "    early_stopper(val_loss, nnet)\n",
    "\n",
    "    # Progress Log\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Stop if early stopping condition met\n",
    "    if early_stopper.early_stop:\n",
    "        print(f'\\nEarly stopping triggered at epoch {epoch+1}\\nBest Validation Loss: {early_stopper.best_loss:.4f}')\n",
    "        break\n",
    "\n",
    "# Reload best model weights from early stopping\n",
    "nnet.load_state_dict(early_stopper.best_model_state)\n",
    "nnet.eval()\n",
    "\n",
    "# 6. Optimize threshold for best F1 Score on validation set\n",
    "probs = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in val_loader:\n",
    "        outputs = nnet(X)\n",
    "        probs.extend(outputs.squeeze().cpu().numpy())\n",
    "        labels.extend(y.cpu().numpy())\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision_vals, recall_vals, thresholds = precision_recall_curve(labels, probs)\n",
    "f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-8)\n",
    "best_threshold = thresholds[f1_scores.argmax()]\n",
    "print(f'Optimal threshold based on validation F1-Score: {best_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9993, Test Precision: 0.8194, Test Recall: 0.7468\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize metrics\n",
    "acc = Accuracy(task='binary')\n",
    "precision = Precision(task='binary')\n",
    "recall = Recall(task='binary')\n",
    "\n",
    "# Model already in evalute mode but a failsafe\n",
    "nnet.eval()\n",
    "\n",
    "# 2. Evaluate model on test set\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        outputs = nnet(X).detach() # Using .detach() should be more memory efficient\n",
    "        preds = (outputs >= best_threshold).float() # Apply the tuned threshold\n",
    "        acc.update(preds, y.view(-1,1))\n",
    "        precision.update(preds, y.view(-1,1))\n",
    "        recall.update(preds, y.view(-1,1))\n",
    "  \n",
    "# 3. Calculate and show metrics for test set\n",
    "test_accuracy = acc.compute()\n",
    "test_precision = precision.compute()\n",
    "test_recall = recall.compute()\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Overfitting Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fighting overfitting\n",
    "- dropout=0.2\n",
    "- learning_rate=1e-3\n",
    "- weight_decay=1e-4\n",
    "- weight decay takes values between 0 and 1.\n",
    "- typically small values like 1e-3.\n",
    "- adds penalty to loss function to discourage large weights and biases.\n",
    "- proportional to the current value of the weight and subtracted from the gradient.\n",
    "- higher the value of the parameter, the less likely the model is to overfit.\n",
    "\n",
    "## Data Augmentation\n",
    "\n",
    "## Maximizing performance\n",
    "- overfit the training set\n",
    "- reduce overfitting\n",
    "- fine-tune hyperparameters\n",
    "\n",
    "### 1. Overfitting Training set\n",
    "- modify training loop to overfit a single data point (batch size = 1)\n",
    "```Python\n",
    "features, labels = next(iter(trainloader))\n",
    "for i in range(1e-3):\n",
    "    outputs = model(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "- Should give an accuracy of 1.0 and a loss of 0\n",
    "- Helps finding bus in code\n",
    "- ***goal***: minimize training loss\n",
    "- create large enough model\n",
    "- hyperparameters kept to defaults for now\n",
    "\n",
    "### 2. Reduce Overfitting\n",
    "- ***Goal***: Maximize the validation accuracy\n",
    "- experiment with:\n",
    "    - Dropout\n",
    "    - Data augmentation\n",
    "    - weight decay\n",
    "    - Reducing model capacity\n",
    "- keep track of each hyperparameter set and corresponding accuracy / metric.\n",
    "- plot each experiment against the default setting train/val curves\n",
    "\n",
    "### 3. Fine-Tuning Hyperparameters\n",
    "- Grid Search:\n",
    "    - Usually done on the optimizer hyperparameters\n",
    "    - Uses values of the parameters at a constant inverval\n",
    "    - Eg. Every momentum value between 0.85 and 0.99 with a constant interval\n",
    "    - \n",
    "```Python\n",
    "for factor in range(2,6):\n",
    "    lr = 10**-factor\n",
    "for val in np.arange(0.85, 1.00, 0.01):\n",
    "    momentum = val\n",
    "```\n",
    "- Random Search:\n",
    "    - Randomly samples parameters between intervals.\n",
    "    - Quicker, and possibly better results as searches a less restricted sapce\n",
    "```Python\n",
    "factor = np.random.uniform(2,6)\n",
    "lr = 10**-factor\n",
    "val = np.random.uniform(0.85, 1.00)\n",
    "momentum = val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
